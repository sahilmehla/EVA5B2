{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Q1. What are Channels and Kernels?\n",
    "\n",
    "A simple 3x3 convolution(done twice sequentially) on a 5x5 input image can be visualized, as shown below:\n",
    "\n",
    "![alt text](https://i.imgur.com/cq71kyV.gif)\n",
    "\n",
    "\n",
    "### Kernel/Filter/Feature Extractor\n",
    "-----------------------------\n",
    "Kernel is the basically a 3x3 matrix(generally) which convolves over the image under study and outputs a image highlighting the particular feature on the input image. Kernel/filer can be used to filter out various features of a image based on texture, edges, patterns.\n",
    "\n",
    "_Each kernel when convolves on an input image produces its own output/channel._\n",
    "\n",
    "\n",
    "\n",
    "### Channels\n",
    "-----------------------------\n",
    "A channel is result of a kernel convolving on an input image. Each kernel produces its own channel. So number of channels in a layer is equal to number of kernels used for convolution.\n",
    "\n",
    "## Q2. Why should we (nearly) always use 3x3 kernels?\n",
    "\n",
    "- 3x3 kernels helps us to accelerate the training of DNN models, this is because the number of parameters used with 3x3 kernels is less.\n",
    "- Another reason for acclerated training is that the NVIDIA has accelerated is hardware for 3x3 convolutions.\n",
    "- By using 3x3 we can create multiple variation of edges, as compared to 2x2\n",
    "- 3x3 is preferred as compared to 2x2 or 4x4 because it provide the centre of axis.\n",
    "- Sequential convolution with multiple 3x3's can be used to achive higher global receptive field which is basically equivallent to lets say 5x5 or 7x7 or 11x11 ... but with lower number of parameters.\n",
    "\n",
    "For example,\n",
    "```\n",
    "3x3 => 3x3 => 3x3 => 3x3    is equivalent to          9x9\n",
    "``` \n",
    "\n",
    "## Q3. How many times to we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)\n",
    "\n",
    "We need to perform 3x3 convolutions 199 times to reach to 1x1 from 199x199.\n",
    "\n",
    "199x199 --> 197x197 --> 195x195 --> 193x193 --> 191x191 --> 189x189 --> 187x187 --> 185x185 --> 183x183 --> 181x181 --> 179x179 --> 177x177 --> 175x175 --> 173x173 --> 171x171 --> 169x169 --> 167x167 --> 165x165 --> 163x163 --> 161x161 --> 159x159 --> 157x157 --> 155x155 --> 153x153 --> 151x151 --> 149x149 --> 147x147 --> 145x145 --> 143x143 --> 141x141 --> 139x139 --> 137x137 --> 135x135 --> 133x133 --> 131x131 --> 129x129 --> 127x127 --> 125x125 --> 123x123 --> 121x121 --> 119x119 --> 117x117 --> 115x115 --> 113x113 --> 111x111 --> 109x109 --> 107x107 --> 105x105 --> 103x103 --> 101x101 --> 99x99 --> 97x97 --> 95x95 --> 93x93 --> 91x91 --> 89x89 --> 87x87 --> 85x85 --> 83x83 --> 81x81 --> 79x79 --> 77x77 --> 75x75 --> 73x73 --> 71x71 --> 69x69 --> 67x67 --> 65x65 --> 63x63 --> 61x61 --> 59x59 --> 57x57 --> 55x55 --> 53x53 --> 51x51 --> 49x49 --> 47x47 --> 45x45 --> 43x43 --> 41x41 --> 39x39 --> 37x37 --> 35x35 --> 33x33 --> 31x31 --> 29x29 --> 27x27 --> 25x25 --> 23x23 --> 21x21 --> 19x19 --> 17x17 --> 15x15 --> 13x13 --> 11x11 --> 9x9 --> 7x7 --> 5x5 --> 3x3 --> 1x1\n",
    "\n",
    "\n",
    "## Q4 How are kernels initialized?\n",
    "\n",
    "Kernels for a DNN are intialized randomly, and the kernel and the other parameters are changed during the training of the model to achive the minimum cost function via Backpropagation\n",
    "\n",
    "## Q5 What happens during the training of a DNN?\n",
    "\n",
    "Most of the state of art neural networks are developed into four blocks(Each block consists of multiple layers.)\n",
    "\n",
    "```\n",
    "Block1                         Block2                         Block3                   Block4\n",
    "(For extracting       ===>   (For extracting          ===>  (For extracting     ===>  (For extracting\n",
    "edges and gradients)          Textures and Patterns)         part of objects)          Objects)\n",
    "```\n",
    "\n",
    "When we train the neural network, all the parameters of the network are randomly intialized and training is started on the training dataset. For each prediction, the cost of the network is evaluated and based on the cost the parameters are adjusted. This algorithm is called backpropagation. The goal behind the whole training exercise is to optimize the cost function so that the final network can predict with best accuracy.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}