{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitdeeplearningcondac257645fc1fb45c18f4c5a71b36562b4",
   "display_name": "Python 3.8.5 64-bit ('deeplearning': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Importing Libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim  # optimizer used to update the weights\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "source": [
    "# Checking the GPU availability and properties"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.7.0\n0.8.1\nTrue\n0\n<torch.cuda.device object at 0x7fab49d57460>\n1\nGeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Check for detecting if GPU is available\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# GPU device details\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# setting the device variable to use GPU if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "source": [
    "# E-T-L Step\n",
    "\n",
    "Extract ==> Transform ==> Load"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting/Downloading and transforming the images to tensor\n",
    "train_set = torchvision.datasets.EMNIST(\n",
    "    root='./data/EMNIST'\n",
    "    ,split='byclass'\n",
    "    ,train=True\n",
    "    ,download=True\n",
    "    ,transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Loading the training images in the form of batches.\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "source": [
    "# Examine the training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "697932"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Max Label id:  61\nMin Label id:  0\nImages in each label:  tensor([34585, 38374, 34203, 35143, 33535, 31416, 34232, 35754, 33946, 33847,  6407,  3878, 10094,  4562,  4934,  9182,\n         2517,  3152, 11946,  3762,  2468,  5076,  9002,  8237, 24983,  8347,  2605,  5073, 20764,  9820, 12602,  4637,\n         4695,  2771,  4743,  2701, 10033,  5159,  2854, 10177, 24631,  2561,  3687,  8738,  2725,  1896,  2491, 15318,\n         2645, 11418,  2749,  2448,  2994, 14105,  2699, 18262,  2830,  2910,  2697,  2822,  2365,  2725],\n       device='cpu')\n"
     ]
    }
   ],
   "source": [
    "print('Max Label id: ',train_set.train_labels.max().item())\n",
    "print('Min Label id: ',train_set.train_labels.min().item())\n",
    "print('Images in each label: ',train_set.train_labels.bincount())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 28, 28]), 35)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "sample = next(iter(train_set))\n",
    "image, label = sample \n",
    "\n",
    "image.shape, label"
   ]
  },
  {
   "source": [
    "# Design the training model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=20, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=20, out_channels=30, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=30, out_channels=62, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, t):                                           #input size >> output size >> receptive field\n",
    "        # (1) Input layer\n",
    "        t=t                                                         #1x28x28 >> 1x28x28 >> 0\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t=self.conv1(t)                                             #1x28x28 >> 10x28x28 >> 3\n",
    "        t=F.relu(t)    \n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t=self.conv2(t)                                             #10x28x28 >> 10x28x28 >> 5 \n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t, kernel_size=2, stride=2)                  #10x28x28 >> 10x14x14 >> 10\n",
    "\n",
    "        # (4) hidden conv layer\n",
    "        t=self.conv3(t)                                             #10x14x14 >> 20x14x14 >> 12\n",
    "        t=F.relu(t)    \n",
    "\n",
    "        # (5) hidden conv layer\n",
    "        t=self.conv4(t)                                             #20x14x14 >> 20x14x14 >> 14\n",
    "        t=F.relu(t)\n",
    "        t=F.max_pool2d(t, kernel_size=2, stride=2)                  #20x14x14 >> 20x7x7 >> 28\n",
    "\n",
    "        # (6) hidden conv layer\n",
    "        t=self.conv5(t)                                             #20x7x7 >> 30x7x7 >> 30\n",
    "        t=F.relu(t)    \n",
    "\n",
    "        # (7) hidden conv layer\n",
    "        t=self.conv6(t)                                             #30x7x7 >> 62x7x7 >> 32\n",
    "\n",
    "        # (8) output layer\n",
    "        t=F.adaptive_avg_pool2d(t, (1, 1))                          #62x7x7 >> 62x1x1 \n",
    "        t=t.squeeze()                                               #62x1x1 >> 62\n",
    "        return t\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Verifying the Network output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([62])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0006, -0.0251,  0.0529,  0.0658,  0.0757, -0.0080,  0.0377, -0.0408, -0.0607, -0.0263, -0.0050, -0.0620,\n",
       "        -0.0209, -0.0246, -0.0523,  0.0619,  0.0031,  0.0220,  0.0417, -0.0140,  0.0629,  0.0368,  0.0492,  0.0819,\n",
       "         0.0468, -0.0538,  0.0130, -0.0341,  0.0007,  0.0638,  0.0143,  0.0128,  0.0400,  0.0377, -0.0140, -0.0446,\n",
       "         0.0558,  0.0193,  0.0055,  0.0321,  0.0551, -0.0318,  0.0426, -0.0188,  0.0263, -0.0137, -0.0343, -0.0323,\n",
       "        -0.0234, -0.0203,  0.0121,  0.0659,  0.0079, -0.0441,  0.0222, -0.0132, -0.0139,  0.0339,  0.0311,  0.0387,\n",
       "        -0.0300,  0.0147], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "network1 = Network()\n",
    "network1.to(device)\n",
    "\n",
    "image = image.reshape(1,1,28,28)\n",
    "image = image.to(device)\n",
    "t1 = network1(image)\n",
    "print(t1.shape)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "source": [
    "# Training the neural network \n",
    "\n",
    "To train a network, need to follow the following process\n",
    "\n",
    "1. create a batch of images\n",
    "2. calculate the loss function(cross entropy loss function)\n",
    "3. calculate the gradient(backprop)\n",
    "4. update the weights to reduce the loss(optimizer)\n",
    "5. repeat step 1 to 4 for all the batches untill you reach 1 epoch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch: 0 total correct: 503118 total loss: tensor(2498.7554, grad_fn=<AddBackward0>)\n",
      "epoch: 1 total correct: 570040 total loss: tensor(1470.7184, grad_fn=<AddBackward0>)\n",
      "epoch: 2 total correct: 574923 total loss: tensor(1402.8359, grad_fn=<AddBackward0>)\n",
      "epoch: 3 total correct: 577396 total loss: tensor(1370.0636, grad_fn=<AddBackward0>)\n",
      "epoch: 4 total correct: 578769 total loss: tensor(1347.6989, grad_fn=<AddBackward0>)\n",
      "epoch: 5 total correct: 579763 total loss: tensor(1333.6880, grad_fn=<AddBackward0>)\n",
      "epoch: 6 total correct: 580950 total loss: tensor(1321.4247, grad_fn=<AddBackward0>)\n",
      "epoch: 7 total correct: 581647 total loss: tensor(1309.9242, grad_fn=<AddBackward0>)\n",
      "epoch: 8 total correct: 582040 total loss: tensor(1302.4614, grad_fn=<AddBackward0>)\n",
      "epoch: 9 total correct: 582975 total loss: tensor(1293.4292, grad_fn=<AddBackward0>)\n",
      "epoch: 10 total correct: 583089 total loss: tensor(1287.7362, grad_fn=<AddBackward0>)\n",
      "epoch: 11 total correct: 583535 total loss: tensor(1278.7551, grad_fn=<AddBackward0>)\n",
      "epoch: 12 total correct: 583684 total loss: tensor(1275.2830, grad_fn=<AddBackward0>)\n",
      "epoch: 13 total correct: 584243 total loss: tensor(1269.1918, grad_fn=<AddBackward0>)\n",
      "epoch: 14 total correct: 585010 total loss: tensor(1261.6180, grad_fn=<AddBackward0>)\n",
      "epoch: 15 total correct: 585477 total loss: tensor(1257.5599, grad_fn=<AddBackward0>)\n",
      "epoch: 16 total correct: 583327 total loss: tensor(1280.1752, grad_fn=<AddBackward0>)\n",
      "epoch: 17 total correct: 585817 total loss: tensor(1248.2745, grad_fn=<AddBackward0>)\n",
      "epoch: 18 total correct: 585973 total loss: tensor(1245.3707, grad_fn=<AddBackward0>)\n",
      "epoch: 19 total correct: 586328 total loss: tensor(1241.6377, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "network.to(device)\n",
    "\n",
    "optimizer=optim.Adam(network.parameters(), lr=0.01)  # Using adam optimizer\n",
    "\n",
    "for epoch in range(20):      # 1 epoch is training the network through all the images once, we have 20 epochs here \n",
    "    \n",
    "    total_loss =0\n",
    "    total_correct=0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        preds = network(images) # pass batch of images through the network\n",
    "        loss=F.cross_entropy(preds, labels) # calculate the loss\n",
    "\n",
    "        # pytorch accumulates the gradients\n",
    "        # so zero out the gradients before calculating again\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() # calculate gradient\n",
    "        optimizer.step() # update weights\n",
    "\n",
    "        total_loss += loss\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "\n",
    "    print(\"epoch:\", epoch, \"total correct:\", total_correct, 'total loss:', total_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8400933042187491"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#Accuracy\n",
    "total_correct/len(train_set)"
   ]
  }
 ]
}